{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a query for the available news over the last day, sorted by popularity\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import os, os.path\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.sparse import coo_matrix\n",
    "from numpy import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_list = pd.read_csv(\"sources.csv\")\n",
    "ids = pd.read_csv(\"ids.csv\")\n",
    "background = \"background.csv\"\n",
    "\n",
    "\n",
    "\n",
    "def get_data(q,key,sources,pagesize=100,from_date=\"2021-02-07\",to_date=\"2021-03-04\"):\n",
    "    \"\"\"q = trump AND (obama OR hillary)\"\"\" \n",
    "    done=False\n",
    "    page = 1\n",
    "    tot_resp = []\n",
    "\n",
    "    while not done:\n",
    "        url = ('http://newsapi.org/v2/everything?'\n",
    "           'q='+q+'&'\n",
    "           f'pageSize={pagesize}&'\n",
    "           f'page={page}&'\n",
    "           f'from={from_date}&'\n",
    "           f'to={to_date}&'\n",
    "            f'sources={sources}&'\n",
    "           'language=en&'\n",
    "           'sortBy=popularity&'\n",
    "           f'apiKey={key}')\n",
    "        response = requests.get(url)\n",
    "        time.sleep(2)\n",
    "        r = response.json()\n",
    "        try:\n",
    "            print(r[\"message\"])\n",
    "        except:\n",
    "            pass\n",
    "        tot_resp.extend(r[\"articles\"])\n",
    "\n",
    "        pages = np.ceil(r[\"totalResults\"]/int(pagesize))\n",
    "\n",
    "        if pages>100:\n",
    "            break\n",
    "            \n",
    "        if page==pages:\n",
    "            done=True\n",
    "            \n",
    "        if page==1:\n",
    "            print(\"results:\",r[\"totalResults\"])\n",
    "#             break\n",
    "        page+=1\n",
    "        \n",
    "    return tot_resp\n",
    "\n",
    "\n",
    "def get_today_data(key):\n",
    "    rand_source = pd.Series(ids).sample(frac=1).reset_index(drop=True)\n",
    "    source_batch_1 = \",\".join(rand_source.iloc[:int(len(rand_source)*0.5)].tolist())\n",
    "    source_batch_2 = \",\".join(rand_source.iloc[int(len(rand_source)*0.5):].tolist())\n",
    "    \n",
    "    \n",
    "    months = [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
    "    date = time.ctime(time.time())\n",
    "    day = (\"0\"+str(int(date[8:10])-1))[-2:]\n",
    "    month = (\"0\"+str(int(months.index(date[4:7]))+1))[-2:]\n",
    "    year = date[-4:]\n",
    "    date = year+\"-\"+month+\"-\"+day\n",
    "\n",
    "    # date = \"2021-03-03\"\n",
    "    d1 = get_data(\"\",key,source_batch_1,from_date=date,to_date=date)\n",
    "    d2 =  get_data(\"\",key,source_batch_2,from_date=date,to_date=date)\n",
    "    \n",
    "    tot_data = d1+d2\n",
    "    \n",
    "    df = pd.DataFrame(tot_data)\n",
    "    df[\"text\"] = [str(i).replace(\"nan\",\"\")+\" \"+str(j).replace(\"nan\",\"\") for i,j in zip(df[\"title\"],df[\"description\"])]\n",
    "    df[\"name\"] = [i[\"name\"] for i in df[\"source\"]]\n",
    "    df[\"id\"] = [i[\"id\"] for i in df[\"source\"]]\n",
    "    del df[\"source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = \"background.csv\"\n",
    "news = df\n",
    "# import the kaggle news_ref\n",
    "news_ref = pd.read_csv(background)\n",
    "\n",
    "# rename the text column, shorten\n",
    "# news_ref = news_ref.rename(columns={\"content\":'text'})\n",
    "# news_ref = news_ref.head(1000)\n",
    "\n",
    "# get the API-obtained news articles (here it's just kaggle)\n",
    "#     news = pd.read_csv(filename)\n",
    "#     news = news.rename(columns={\"content\":'text'})\n",
    "#     news = news.head(10)\n",
    "\n",
    "# get the word count for each article\n",
    "news_ref['word_count'] = news_ref['text'].apply(lambda x: len(x.split(\" \")))\n",
    "news['word_count'] = news['text'].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "# creating a list of stopwords and adding custom stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "new_words = []\n",
    "stop_words = stop_words.union(new_words)\n",
    "\n",
    "# create a corpus to store the words in\n",
    "corpus_ref = []\n",
    "corpus = []\n",
    "\n",
    "# clean the text (reference)\n",
    "for i in range(len(news_ref)):\n",
    "\n",
    "    # remove punctutation\n",
    "    text = re.sub('[^a-zA-Z]',' ', news_ref['text'][i])\n",
    "\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove tags\n",
    "    text = re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "\n",
    "    # remove special characters and digits\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "\n",
    "    # convert to list from string\n",
    "    text = text.split()\n",
    "\n",
    "    # stem\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    # Lemmatisation\n",
    "    lem = WordNetLemmatizer()\n",
    "    text = [lem.lemmatize(word) for word in text if not word in stop_words]\n",
    "    text = \" \".join(text)\n",
    "    corpus_ref.append(text)\n",
    "\n",
    "# clean the text (API)\n",
    "for i in range(len(news)):\n",
    "\n",
    "    # remove punctutation\n",
    "    text = re.sub('[^a-zA-Z]',' ', news['text'][i])\n",
    "\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove tags\n",
    "    text = re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "\n",
    "    # remove special characters and digits\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "\n",
    "    # convert to list from string\n",
    "    text = text.split()\n",
    "\n",
    "    text = [lem.lemmatize(word) for word in text if not word in stop_words]\n",
    "    text = \" \".join(text)\n",
    "    corpus.append(text)\n",
    "\n",
    "# get the vocabulary keys, set tf-idf parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vocabulary keys, set tf-idf parameters\n",
    "cv = CountVectorizer(max_df = .8,stop_words=stop_words,max_features=10000, ngram_range=(1,3))\n",
    "X = cv.fit_transform(corpus+corpus_ref)\n",
    "\n",
    "# start tf-idf\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(X)\n",
    "\n",
    "# get feature names from the kaggle news_ref\n",
    "feature_names=cv.get_feature_names()\n",
    "\n",
    "\n",
    "\n",
    "def checkForRepeats(keywords,conf):\n",
    "    words = []\n",
    "    confs = []\n",
    "    repeat = 0\n",
    "    for i in range(len(keywords)):\n",
    "        for j in range(len(keywords)):\n",
    "            if (i != j) and (keywords[i] in keywords[j]):\n",
    "                repeat = 1\n",
    "        if repeat == 0:\n",
    "            words.append(keywords[i])\n",
    "            confs.append(conf[i])\n",
    "        repeat = 0\n",
    "    return [words, confs]\n",
    "\n",
    "# set number of articles\n",
    "n_articles = len(news)\n",
    "\n",
    "# set number of keywords\n",
    "n_keywords = 5\n",
    "\n",
    "\n",
    "\n",
    "# initialize the sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# function for sorting tf_idf in descending order\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "# get the feature names and tf-idf score of top n items\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "\n",
    "    # use only top n items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "\n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "\n",
    "        # keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    # create a tuples of feature,score\n",
    "    # results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "\n",
    "    return results\n",
    "\n",
    "# create lists to return results\n",
    "words = []\n",
    "sentiments = []\n",
    "confs = []\n",
    "\n",
    "# iterate through the articles to get keywords and sentiment\n",
    "for i in range(n_articles):\n",
    "\n",
    "    # fetch document for which keywords needs to be extracted\n",
    "    doc=corpus[i]\n",
    "\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "\n",
    "    # sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    # extract only the top n; n here is 10\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,n_keywords)\n",
    "\n",
    "    # convert keywords to format, check for repeats\n",
    "    keys = list(keywords.items())\n",
    "    conf = [i[1] for i in keys]\n",
    "    keys = [i[0] for i in keys]\n",
    "    [keys, conf] = checkForRepeats(keys,conf)\n",
    "\n",
    "    # gets the sentiment\n",
    "    sent = sia.polarity_scores(corpus[i])\n",
    "    sent = sent.get('compound')\n",
    "\n",
    "    # add to lists\n",
    "    words.append(keys)\n",
    "    sentiments.append(sent)\n",
    "    confs.append(conf)\n",
    "\n",
    "\n",
    "# add outputs to pandas database    \n",
    "outputs = pd.DataFrame({'sentiment': sentiments, 'keywords': words,'conf':confs})\n",
    "news = pd.concat([news,outputs],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity of the articles we want to look at\n",
    "def get_cosine_sim(*strs): \n",
    "    vectors = [t for t in get_vectors(*strs)]\n",
    "    return cosine_similarity(vectors)\n",
    "\n",
    "def get_vectors(*strs):\n",
    "    text = [t for t in strs]\n",
    "    vectorizer = CountVectorizer(text)\n",
    "    vectorizer.fit(text)\n",
    "    return vectorizer.transform(text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = get_cosine_sim(*[\" \".join(i) for i in news[\"keywords\"]])\n",
    "sim = []\n",
    "long_list = []\n",
    "match_dict = defaultdict(list)\n",
    "match_dict_l = defaultdict(list)\n",
    "\n",
    "limit = 0.2\n",
    "for i in range(n_articles):\n",
    "    for j,ind in zip(sims[i],range(len(sims[i]))):\n",
    "        if j>limit and j<0.95:\n",
    "            match_dict[i].append([ind])\n",
    "            match_dict_l[i].append([j])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the cosine sim for 9 articles (the max we'd be doing)\n",
    "sims = get_cosine_sim(*[\" \".join(i) for i in news[\"keywords\"]])\n",
    "sim = []\n",
    "long_list = []\n",
    "match_dict = defaultdict(list)\n",
    "match_dict_l = defaultdict(list)\n",
    "\n",
    "limit = 0.2\n",
    "for i in range(n_articles):\n",
    "    for j,ind in zip(sims[i],range(len(sims[i]))):\n",
    "        if j>limit and j<0.95:\n",
    "            match_dict[i].append(ind)\n",
    "            match_dict_l[i].append(j)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in news.iterrows():\n",
    "    if len(match_dict[i[0]])>9:\n",
    "        print(i[1][\"title\"])\n",
    "        for j in match_dict[i[0]]:\n",
    "            print(news[\"title\"].iloc[j])\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
